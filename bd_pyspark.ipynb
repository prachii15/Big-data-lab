{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BD Assignment - Pyspark\n",
        "## Prachi Mehta (202318008)"
      ],
      "metadata": {
        "id": "apI3abiAP6Fw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVQDMbJgRN0E",
        "outputId": "ec6fcdaf-024b-42ec-9d60-e1132ce031a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=83a6bd599f173b9b72d4348876bcd6409faf8e7a5b4a578b977624bb697aa56a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "52ko-zSYRsOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RDD and dataframe manipulation\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(10)\n",
        "\n",
        "# Generate 100 random numbers in range 0 to 10 using numpy\n",
        "random_numbers = np.random.randint(0, 11, 100)\n",
        "print(random_numbers)\n",
        "\n",
        "# Create RDD using parallelize function\n",
        "rdd = spark.sparkContext.parallelize(random_numbers)\n",
        "\n",
        "# Map each number to a tuple of (number, 1)\n",
        "mapped_rdd = rdd.map(lambda x: (x, 1))\n",
        "\n",
        "# Reduce by key to sum up the occurrences of each number\n",
        "reduced_rdd = mapped_rdd.reduceByKey(int.__add__)\n",
        "\n",
        "# Sort by key\n",
        "sorted_rdd = reduced_rdd.sortByKey()\n",
        "\n",
        "# Print frequency of each number\n",
        "for num, freq in sorted_rdd.collect():\n",
        "    print(f\"Number {num}: {freq} occurrences\")\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNGECMqi80T2",
        "outputId": "416bea57-faa2-45ca-d8be-7856d2ff547c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9  4  0  1  9  0  1 10  8  9  0 10  8  6  4  3  0  4  6  8 10  1  8  4\n",
            "  1  3  6  5  3  9  6  9  1  9  4  2  6  7  8 10  8  9  2  0  6  7  8  1\n",
            "  7  1  4 10  0  8  5  4  7  8  8  2  6  2  8  8  6  6  5 10  6  0  0  6\n",
            "  9  1  8 10  9  1  2  8  9  9  5  0  2  7  3  0  4  2  0  3  3  1  2  5\n",
            "  9  0 10  1]\n",
            "Number 0: 12 occurrences\n",
            "Number 1: 11 occurrences\n",
            "Number 2: 8 occurrences\n",
            "Number 3: 6 occurrences\n",
            "Number 4: 8 occurrences\n",
            "Number 5: 5 occurrences\n",
            "Number 6: 11 occurrences\n",
            "Number 7: 5 occurrences\n",
            "Number 8: 14 occurrences\n",
            "Number 9: 12 occurrences\n",
            "Number 10: 8 occurrences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, lower, split\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Word Frequency Calculation\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load text8 dataset into a DataFrame\n",
        "text8_df = spark.read.text(\"text8.unknown\")\n",
        "\n",
        "# Extract words, convert them to lowercase, split them, and explode them into separate rows\n",
        "words_df = text8_df.select(explode(split(lower(text8_df.value), \" \")).alias(\"word\"))\n",
        "\n",
        "# Filter words containing the letter 'a'\n",
        "words_with_a_df = words_df.filter(words_df.word.like(\"%a%\"))\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_frequencies_df = words_with_a_df.groupBy(\"word\").count()\n",
        "\n",
        "# Show the frequencies of words containing the letter 'a'\n",
        "word_frequencies_df.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whuzk-Wo9u6p",
        "outputId": "26ee58b4-4b6e-46cf-91c8-c455876ca2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|         word|count|\n",
            "+-------------+-----+\n",
            "|  interaction|   43|\n",
            "|      marxism|    8|\n",
            "|     everyday|   21|\n",
            "|    indicator|   20|\n",
            "|socialization|    4|\n",
            "|  handicapped|    3|\n",
            "|     cautious|    7|\n",
            "|       ransom|   24|\n",
            "|      barrier|   26|\n",
            "|  unequivocal|    1|\n",
            "|       travel|  143|\n",
            "|          art|  631|\n",
            "|     didactic|    1|\n",
            "|       lamian|    1|\n",
            "|        trail|   25|\n",
            "|    arguments|  105|\n",
            "|        oscar|   65|\n",
            "|    librarian|    1|\n",
            "|  mccarthyism|    6|\n",
            "|    standards|   92|\n",
            "+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, corr\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Iris Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load iris JSON data into a DataFrame\n",
        "iris_df = spark.read.json(\"iris.json\")\n",
        "\n",
        "# Calculate Pearson Correlation between petalLength and petalWidth\n",
        "pearson_correlation = iris_df.select(corr(\"petalLength\", \"petalWidth\")).collect()[0][0]\n",
        "print(f\"Pearson Correlation between petalLength and petalWidth: {pearson_correlation}\")\n",
        "\n",
        "# Show sepalLength, sepalWidth, and species for rows with petalLength >= 1.4\n",
        "filtered_df = iris_df.filter(col(\"petalLength\") >= 1.4).select(\"sepalLength\", \"sepalWidth\", \"species\")\n",
        "filtered_df.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-xBY5HC-vjf",
        "outputId": "fc1a6e16-769c-4dde-86e2-3db725c4e316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson Correlation between petalLength and petalWidth: 0.962865431402796\n",
            "+-----------+----------+-------+\n",
            "|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-------+\n",
            "|        5.1|       3.5| setosa|\n",
            "|        4.9|       3.0| setosa|\n",
            "|        4.6|       3.1| setosa|\n",
            "|        5.0|       3.6| setosa|\n",
            "|        5.4|       3.9| setosa|\n",
            "|        4.6|       3.4| setosa|\n",
            "|        5.0|       3.4| setosa|\n",
            "|        4.4|       2.9| setosa|\n",
            "|        4.9|       3.1| setosa|\n",
            "|        5.4|       3.7| setosa|\n",
            "|        4.8|       3.4| setosa|\n",
            "|        4.8|       3.0| setosa|\n",
            "|        5.7|       4.4| setosa|\n",
            "|        5.1|       3.5| setosa|\n",
            "|        5.7|       3.8| setosa|\n",
            "|        5.1|       3.8| setosa|\n",
            "|        5.4|       3.4| setosa|\n",
            "|        5.1|       3.7| setosa|\n",
            "|        5.1|       3.3| setosa|\n",
            "|        4.8|       3.4| setosa|\n",
            "+-----------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}